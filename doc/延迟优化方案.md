**核心原则**：本方案通过工具分类、响应模板和动态流程决策，实现交互延迟的极致优化。核心创新在于引入了“三轨并行”模式：对于可预测结果的“操控类”指令，系统**同时**执行设备动作、通过模板进行**即时语音播报**，并**并行地**让大语言模型（LLM）生成一个更完整的总结性回复，用于后续交互或界面展示。

---

### **1. 扩展工具定义 (Extended Tool Definition)**

为支持新的流程，我们修改工具定义，将类型字段扁平化为 `main_type` 和 `sub_type`。

**旧版参考**：
```json
{
    "name": "set_virtual_human_expression",
    "description": "设置AI虚拟人的表情",
    "parameters": {
        "type": "object",
        "properties": {
            "expression": {
                "type": "string",
                "description": "要设置的表情",
                "enum": ["close_eyes_smile", "close_eyes", "amazed", "smile", "cry", "idle"]
            }
        },
        "required": ["expression"]
    }
}
```

**新版工具定义规范**：
在工具的JSON定义中，直接包含 `main_type` 和 `sub_type` 两个顶级字段。

```json
{
    "name": "set_screen_brightness",
    "description": "设置屏幕亮度",
    "main_type": "local",
    "sub_type": "control",
    "parameters": {
        "type": "object",
        "properties": {
            "level": {
                "type": "integer",
                "description": "屏幕亮度，从0到100。0为最暗，100为最亮。"
            }
        },
        "required": ["level"]
    }
}
```

**字段详解**：

*   **`main_type` (字符串)**: 定义工具的执行环境。
    *   `"local"`: 本地设备工具。
    *   `"remote"`: 外部/云端工具。
*   **`sub_type` (字符串)**: 定义工具的交互模式，是流程决策的核心。
    *   `"control"` (操控类): 执行一个动作，结果可被高度预测为“成功”（如开关灯）。
    *   `"query"` (查询类): 获取未知信息，必须等待实际结果（如查天气）。

---

### **2. 系统执行工作流 (System Execution Workflow)**

**步骤 1：初始化与工具注册 (Initialization & Tool Registration)**
1.  工具注册时，调用generate_intent_resp.py返回格式为
["好的，马上为您把屏幕亮度调整到#{level}哈~", "收到！正在将屏幕亮度设置为#{level}，请稍等一下哦！"]数组，
缓存到client_session，key为工具名称，value为数组

**步骤 2：意图识别 (Intent Recognition)**
1.  接收用户输入（例如 "笑一个再把亮度调到100"）。
2.  `意图识别节点`处理后，输出工具调用列表。
    *   **输出示例**:
        ```json
        [
            { "name": "set_expression", "arguments": { "emotion": "smile" } },
            { "name": "set_screen_brightness", "arguments": { "level": 100 } }
        ]
        ```

**步骤 3：流程决策与执行 (Flow Decision & Execution)**

系统检查意图列表中的所有工具调用，根据它们的 `sub_type` 决定执行路径。

#### **决策分支 A：如果所有工具的 `sub_type` 均为 `"control"`**

启动极致优化的 **“三轨并行流”**。

1.  **立刻兵分三路 (使用 `asyncio.gather` 或类似并发机制):**

    *   **轨道 1 (执行流 - Execution Track):**
        *   **任务**: 立即执行设备动作。
        *   **流程**: 根据 `main_type`，异步地向设备端发送所有工具调用指令（`set_expression('smile')` 和 `set_brightness(100)`）。此轨道独立运行，不阻塞其他流程。

    *   **轨道 2 (即时回应流 - Immediate Response Track):**
        *   **任务**: 提供最低延迟的语音反馈。
        *   **流程**:
            a.  遍历工具调用列表。
            b.  为每个调用，从缓存中随机选取一个 `response_templates` 模板。
            c.  将 `arguments` 中的参数值填入模板占位符。
            d.  将生成的文本（如 "收到！正在将屏幕亮度设置为100，请稍等一下哦！"）**立即送入 TTS 引擎进行语音合成并播放**。

    *   **轨道 3 (总结生成流 - Summary Generation Track):**
        *   **任务**: 生成一个更完整、更自然的总结性回复，用于界面显示或对话历史记录。
        *   **流程**:
            a.  **伪造成功结果 (Forge Success Response)**: 为每个工具调用创建一个表示“成功”的、结构化的伪造结果（Function Response）。
                *   **伪造结果示例**:
                    ```json
                    [
                        { "name": "set_expression", "response": { "status": "success" } },
                        { "name": "set_screen_brightness", "response": { "status": "success", "level": 100 } }
                    ]
                    ```
            b.  **调用聊天节点**: 将这个伪造的成功结果列表喂给 `聊天生成节点` (LLM)。
            c.  **生成最终回复**: LLM 基于这些“成功”信息，生成一个自然的、总结性的回复文本，例如：“好的，已经为您调整了表情和亮度！”
            d.  **处理最终回复**: 此回复可用于**屏幕显示**、存入**聊天记录**，或作为未来对话的上下文。

#### **决策分支 B：如果存在至少一个工具的 `sub_type` 为 `"query"`**
在意图识别之后先响应模版
1.  **执行并等待**: 并发执行所有工具调用，但**必须 `await` 等待所有工具返回真实的执行结果**。
2.  **生成回复**: 将所有工具的**真实执行结果**喂给 `聊天生成节点` (LLM)，由LLM生成综合性的回复。
3.  **语音输出**: 将LLM生成的回复文本送入TTS引擎播放。





客户端优化方案
1、增加状态机，唤醒语音或麦克风为最高优先级可以打断音频、视频等播放